{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Natural Language Processing <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Downloader.download of <nltk.downloader.Downloader object at 0x0000024E61532A30>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.1 Text Preprocessing\n",
    "import nltk\n",
    "nltk.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1= 'Text Mining is the process of EXTRACTING or RETRIEVING Information from large text@data???.Text \"\"\"\" preprocessing is the first @step?!!! in text mining'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text mining is the process of extracting or retrieving information from large text@data???.text \"\"\"\" preprocessing is the first @step?!!! in text mining'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### lowercasing \n",
    "text2=text1.lower()\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text mining is the process of extracting or retrieving information from large text@data???.text \"\"\"\" preprocessing is the first @step?!!! in text mining'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### removing whitespace\n",
    "text3=text2.strip()\n",
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "### removing punctuation\n",
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text mining is the process of extracting or retrieving information from large textdatatext  preprocessing is the first step in text mining'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation=\"\".join([char for char in text3 if char not in string.punctuation])\n",
    "remove_punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'mining',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'extracting',\n",
       " 'or',\n",
       " 'retrieving',\n",
       " 'information',\n",
       " 'from',\n",
       " 'large',\n",
       " 'textdatatext',\n",
       " 'preprocessing',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'step',\n",
       " 'in',\n",
       " 'text',\n",
       " 'mining']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##tokenization\n",
    "from nltk import word_tokenize\n",
    "tokenize_words=word_tokenize(remove_punctuation)\n",
    "tokenize_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "### word filtering\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'mining',\n",
       " 'process',\n",
       " 'extracting',\n",
       " 'retrieving',\n",
       " 'information',\n",
       " 'large',\n",
       " 'textdatatext',\n",
       " 'preprocessing',\n",
       " 'first',\n",
       " 'step',\n",
       " 'text',\n",
       " 'mining']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [char for char in tokenize_words if char not in stop_words]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'mine', 'process', 'extract', 'retriev', 'inform', 'larg', 'textdatatext', 'preprocess', 'first', 'step', 'text', 'mine']\n"
     ]
    }
   ],
   "source": [
    "### stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "stemmed=[porter.stem(word) for word in filtered_words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "mining\n",
      "process\n",
      "extracting\n",
      "retrieving\n",
      "information\n",
      "large\n",
      "textdatatext\n",
      "preprocessing\n",
      "first\n",
      "step\n",
      "text\n",
      "mining\n"
     ]
    }
   ],
   "source": [
    "### Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemma_words=[]\n",
    "for word in filtered_words:\n",
    "    lemma_words.append(lemmatizer.lemmatize(word))\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text', 'NN'), ('mining', 'NN'), ('process', 'NN'), ('extracting', 'VBG'), ('retrieving', 'VBG'), ('information', 'NN'), ('large', 'JJ'), ('textdatatext', 'NN'), ('preprocessing', 'VBG'), ('first', 'JJ'), ('step', 'NN'), ('text', 'NN'), ('mining', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Tenisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "### POS Tagger\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "poss=pos_tag(filtered_words)\n",
    "print(poss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2 Bag Of Words <3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bag of Words is a method to extract features from text documents.',\n",
       " 'These features can be used for training machine learning algorithms.',\n",
       " 'It creates a vocabulary of all the unique words occurring in all the documents in the training set.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases=[\"Bag of Words is a method to extract features from text documents.\",\n",
    "         \"These features can be used for training machine learning algorithms.\",\n",
    "         \"It creates a vocabulary of all the unique words occurring in all the documents in the training set.\"]\n",
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer()\n",
    "count_vect.fit(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:29\n",
      "vocabulary content:{'bag': 2, 'of': 18, 'words': 28, 'is': 12, 'method': 16, 'to': 23, 'extract': 7, 'features': 8, 'from': 10, 'text': 20, 'documents': 6, 'these': 22, 'can': 4, 'be': 3, 'used': 26, 'for': 9, 'training': 24, 'machine': 15, 'learning': 14, 'algorithms': 0, 'it': 13, 'creates': 5, 'vocabulary': 27, 'all': 1, 'the': 21, 'unique': 25, 'occurring': 17, 'in': 11, 'set': 19}\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size:{}\".format(len(count_vect.vocabulary_)))\n",
    "print(\"vocabulary content:{}\".format(count_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 28)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 22)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 26)\t1\n",
      "  (2, 1)\t2\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 11)\t2\n",
      "  (2, 13)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 21)\t3\n",
      "  (2, 24)\t1\n",
      "  (2, 25)\t1\n",
      "  (2, 27)\t1\n",
      "  (2, 28)\t1\n"
     ]
    }
   ],
   "source": [
    "bag_of_words=count_vect.transform(phrases)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words as an array:\n",
      "[[0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1]\n",
      " [1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0]\n",
      " [0 2 0 0 0 1 1 0 0 0 0 2 0 1 0 0 0 1 1 1 0 3 0 0 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "##converting bag of words to an array\n",
    "\n",
    "print(\"bag_of_words as an array:\\n{}\".format(bag_of_words.toarray()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words=bag_of_words.toarray()\n",
    "vocabulary=count_vect.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bag</th>\n",
       "      <th>of</th>\n",
       "      <th>words</th>\n",
       "      <th>is</th>\n",
       "      <th>method</th>\n",
       "      <th>to</th>\n",
       "      <th>extract</th>\n",
       "      <th>features</th>\n",
       "      <th>from</th>\n",
       "      <th>text</th>\n",
       "      <th>...</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>it</th>\n",
       "      <th>creates</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>all</th>\n",
       "      <th>the</th>\n",
       "      <th>unique</th>\n",
       "      <th>occurring</th>\n",
       "      <th>in</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bag  of  words  is  method  to  extract  features  from  text  ...  \\\n",
       "0    0   0      1   0       0   0        1         1     1     0  ...   \n",
       "1    1   0      0   1       1   0        0         0     1     1  ...   \n",
       "2    0   2      0   0       0   1        1         0     0     0  ...   \n",
       "\n",
       "   algorithms  it  creates  vocabulary  all  the  unique  occurring  in  set  \n",
       "0           0   1        0           0    1    0       0          0   0    1  \n",
       "1           0   0        0           1    0    1       0          1   0    0  \n",
       "2           1   0        3           0    0    1       1          0   1    1  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(bag_of_words,columns=vocabulary)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3 N gram <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams=CountVectorizer(ngram_range=(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text mining can be broadly defined as a knowledge-intensive process.in which a user interacts with a document collection over time by using various types of analysis tools in Text mining.',\n",
       " 'Text can be both structured and unstructured']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=[\"Text mining can be broadly defined as a knowledge-intensive process.\" \n",
    "      \"in which a user interacts with a document collection over time by using various types of analysis tools in Text mining.\",\n",
    "     \"Text can be both structured and unstructured\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analysis tools', 'analysis tools in', 'analysis tools in text', 'and unstructured', 'as knowledge', 'as knowledge intensive', 'as knowledge intensive process', 'be both', 'be both structured', 'be both structured and', 'be broadly', 'be broadly defined', 'be broadly defined as', 'both structured', 'both structured and', 'both structured and unstructured', 'broadly defined', 'broadly defined as', 'broadly defined as knowledge', 'by using', 'by using various', 'by using various types', 'can be', 'can be both', 'can be both structured', 'can be broadly', 'can be broadly defined', 'collection over', 'collection over time', 'collection over time by', 'defined as', 'defined as knowledge', 'defined as knowledge intensive', 'document collection', 'document collection over', 'document collection over time', 'in text', 'in text mining', 'in which', 'in which user', 'in which user interacts', 'intensive process', 'intensive process in', 'intensive process in which', 'interacts with', 'interacts with document', 'interacts with document collection', 'knowledge intensive', 'knowledge intensive process', 'knowledge intensive process in', 'mining can', 'mining can be', 'mining can be broadly', 'of analysis', 'of analysis tools', 'of analysis tools in', 'over time', 'over time by', 'over time by using', 'process in', 'process in which', 'process in which user', 'structured and', 'structured and unstructured', 'text can', 'text can be', 'text can be both', 'text mining', 'text mining can', 'text mining can be', 'time by', 'time by using', 'time by using various', 'tools in', 'tools in text', 'tools in text mining', 'types of', 'types of analysis', 'types of analysis tools', 'user interacts', 'user interacts with', 'user interacts with document', 'using various', 'using various types', 'using various types of', 'various types', 'various types of', 'various types of analysis', 'which user', 'which user interacts', 'which user interacts with', 'with document', 'with document collection', 'with document collection over']\n",
      "[[1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 2 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "x=n_grams.fit_transform(text)\n",
    "print(n_grams.get_feature_names())\n",
    "x=n_grams.transform(text)\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis tools</th>\n",
       "      <th>analysis tools in</th>\n",
       "      <th>analysis tools in text</th>\n",
       "      <th>and unstructured</th>\n",
       "      <th>as knowledge</th>\n",
       "      <th>as knowledge intensive</th>\n",
       "      <th>as knowledge intensive process</th>\n",
       "      <th>be both</th>\n",
       "      <th>be both structured</th>\n",
       "      <th>be both structured and</th>\n",
       "      <th>be broadly</th>\n",
       "      <th>be broadly defined</th>\n",
       "      <th>be broadly defined as</th>\n",
       "      <th>both structured</th>\n",
       "      <th>both structured and</th>\n",
       "      <th>both structured and unstructured</th>\n",
       "      <th>broadly defined</th>\n",
       "      <th>broadly defined as</th>\n",
       "      <th>broadly defined as knowledge</th>\n",
       "      <th>by using</th>\n",
       "      <th>by using various</th>\n",
       "      <th>by using various types</th>\n",
       "      <th>can be</th>\n",
       "      <th>can be both</th>\n",
       "      <th>can be both structured</th>\n",
       "      <th>can be broadly</th>\n",
       "      <th>can be broadly defined</th>\n",
       "      <th>collection over</th>\n",
       "      <th>collection over time</th>\n",
       "      <th>collection over time by</th>\n",
       "      <th>defined as</th>\n",
       "      <th>defined as knowledge</th>\n",
       "      <th>defined as knowledge intensive</th>\n",
       "      <th>document collection</th>\n",
       "      <th>document collection over</th>\n",
       "      <th>document collection over time</th>\n",
       "      <th>in text</th>\n",
       "      <th>in text mining</th>\n",
       "      <th>in which</th>\n",
       "      <th>in which user</th>\n",
       "      <th>in which user interacts</th>\n",
       "      <th>intensive process</th>\n",
       "      <th>intensive process in</th>\n",
       "      <th>intensive process in which</th>\n",
       "      <th>interacts with</th>\n",
       "      <th>interacts with document</th>\n",
       "      <th>interacts with document collection</th>\n",
       "      <th>knowledge intensive</th>\n",
       "      <th>knowledge intensive process</th>\n",
       "      <th>knowledge intensive process in</th>\n",
       "      <th>mining can</th>\n",
       "      <th>mining can be</th>\n",
       "      <th>mining can be broadly</th>\n",
       "      <th>of analysis</th>\n",
       "      <th>of analysis tools</th>\n",
       "      <th>of analysis tools in</th>\n",
       "      <th>over time</th>\n",
       "      <th>over time by</th>\n",
       "      <th>over time by using</th>\n",
       "      <th>process in</th>\n",
       "      <th>process in which</th>\n",
       "      <th>process in which user</th>\n",
       "      <th>structured and</th>\n",
       "      <th>structured and unstructured</th>\n",
       "      <th>text can</th>\n",
       "      <th>text can be</th>\n",
       "      <th>text can be both</th>\n",
       "      <th>text mining</th>\n",
       "      <th>text mining can</th>\n",
       "      <th>text mining can be</th>\n",
       "      <th>time by</th>\n",
       "      <th>time by using</th>\n",
       "      <th>time by using various</th>\n",
       "      <th>tools in</th>\n",
       "      <th>tools in text</th>\n",
       "      <th>tools in text mining</th>\n",
       "      <th>types of</th>\n",
       "      <th>types of analysis</th>\n",
       "      <th>types of analysis tools</th>\n",
       "      <th>user interacts</th>\n",
       "      <th>user interacts with</th>\n",
       "      <th>user interacts with document</th>\n",
       "      <th>using various</th>\n",
       "      <th>using various types</th>\n",
       "      <th>using various types of</th>\n",
       "      <th>various types</th>\n",
       "      <th>various types of</th>\n",
       "      <th>various types of analysis</th>\n",
       "      <th>which user</th>\n",
       "      <th>which user interacts</th>\n",
       "      <th>which user interacts with</th>\n",
       "      <th>with document</th>\n",
       "      <th>with document collection</th>\n",
       "      <th>with document collection over</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis tools  analysis tools in  analysis tools in text  \\\n",
       "0               1                  1                       1   \n",
       "1               0                  0                       0   \n",
       "\n",
       "   and unstructured  as knowledge  as knowledge intensive  \\\n",
       "0                 0             1                       1   \n",
       "1                 1             0                       0   \n",
       "\n",
       "   as knowledge intensive process  be both  be both structured  \\\n",
       "0                               1        0                   0   \n",
       "1                               0        1                   1   \n",
       "\n",
       "   be both structured and  be broadly  be broadly defined  \\\n",
       "0                       0           1                   1   \n",
       "1                       1           0                   0   \n",
       "\n",
       "   be broadly defined as  both structured  both structured and  \\\n",
       "0                      1                0                    0   \n",
       "1                      0                1                    1   \n",
       "\n",
       "   both structured and unstructured  broadly defined  broadly defined as  \\\n",
       "0                                 0                1                   1   \n",
       "1                                 1                0                   0   \n",
       "\n",
       "   broadly defined as knowledge  by using  by using various  \\\n",
       "0                             1         1                 1   \n",
       "1                             0         0                 0   \n",
       "\n",
       "   by using various types  can be  can be both  can be both structured  \\\n",
       "0                       1       1            0                       0   \n",
       "1                       0       1            1                       1   \n",
       "\n",
       "   can be broadly  can be broadly defined  collection over  \\\n",
       "0               1                       1                1   \n",
       "1               0                       0                0   \n",
       "\n",
       "   collection over time  collection over time by  defined as  \\\n",
       "0                     1                        1           1   \n",
       "1                     0                        0           0   \n",
       "\n",
       "   defined as knowledge  defined as knowledge intensive  document collection  \\\n",
       "0                     1                               1                    1   \n",
       "1                     0                               0                    0   \n",
       "\n",
       "   document collection over  document collection over time  in text  \\\n",
       "0                         1                              1        1   \n",
       "1                         0                              0        0   \n",
       "\n",
       "   in text mining  in which  in which user  in which user interacts  \\\n",
       "0               1         1              1                        1   \n",
       "1               0         0              0                        0   \n",
       "\n",
       "   intensive process  intensive process in  intensive process in which  \\\n",
       "0                  1                     1                           1   \n",
       "1                  0                     0                           0   \n",
       "\n",
       "   interacts with  interacts with document  \\\n",
       "0               1                        1   \n",
       "1               0                        0   \n",
       "\n",
       "   interacts with document collection  knowledge intensive  \\\n",
       "0                                   1                    1   \n",
       "1                                   0                    0   \n",
       "\n",
       "   knowledge intensive process  knowledge intensive process in  mining can  \\\n",
       "0                            1                               1           1   \n",
       "1                            0                               0           0   \n",
       "\n",
       "   mining can be  mining can be broadly  of analysis  of analysis tools  \\\n",
       "0              1                      1            1                  1   \n",
       "1              0                      0            0                  0   \n",
       "\n",
       "   of analysis tools in  over time  over time by  over time by using  \\\n",
       "0                     1          1             1                   1   \n",
       "1                     0          0             0                   0   \n",
       "\n",
       "   process in  process in which  process in which user  structured and  \\\n",
       "0           1                 1                      1               0   \n",
       "1           0                 0                      0               1   \n",
       "\n",
       "   structured and unstructured  text can  text can be  text can be both  \\\n",
       "0                            0         0            0                 0   \n",
       "1                            1         1            1                 1   \n",
       "\n",
       "   text mining  text mining can  text mining can be  time by  time by using  \\\n",
       "0            2                1                   1        1              1   \n",
       "1            0                0                   0        0              0   \n",
       "\n",
       "   time by using various  tools in  tools in text  tools in text mining  \\\n",
       "0                      1         1              1                     1   \n",
       "1                      0         0              0                     0   \n",
       "\n",
       "   types of  types of analysis  types of analysis tools  user interacts  \\\n",
       "0         1                  1                        1               1   \n",
       "1         0                  0                        0               0   \n",
       "\n",
       "   user interacts with  user interacts with document  using various  \\\n",
       "0                    1                             1              1   \n",
       "1                    0                             0              0   \n",
       "\n",
       "   using various types  using various types of  various types  \\\n",
       "0                    1                       1              1   \n",
       "1                    0                       0              0   \n",
       "\n",
       "   various types of  various types of analysis  which user  \\\n",
       "0                 1                          1           1   \n",
       "1                 0                          0           0   \n",
       "\n",
       "   which user interacts  which user interacts with  with document  \\\n",
       "0                     1                          1              1   \n",
       "1                     0                          0              0   \n",
       "\n",
       "   with document collection  with document collection over  \n",
       "0                         1                              1  \n",
       "1                         0                              0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "df=pd.DataFrame(x.toarray(),columns=n_grams.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.4 Vector Space Models <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn .feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text preprocessing Tasks include all those routines, processes, and methods required to prepare data for a text mining.',\n",
       " 'Text Analytics is the process of drawing meaning out of written communication.',\n",
       " 'In a customer experience context, text analytics means examining text that was written by, or about, customers.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data=[\"Text preprocessing Tasks include all those routines, processes, and methods required to prepare data for a text mining.\",\n",
    "\"Text Analytics is the process of drawing meaning out of written communication.\",\n",
    "\"In a customer experience context, text analytics means examining text that was written by, or about, customers.\"]\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'all', 'analytics', 'and', 'by', 'communication', 'context', 'customer', 'customers', 'data', 'drawing', 'examining', 'experience', 'for', 'in', 'include', 'is', 'meaning', 'means', 'methods', 'mining', 'of', 'or', 'out', 'prepare', 'preprocessing', 'process', 'processes', 'required', 'routines', 'tasks', 'text', 'that', 'the', 'those', 'to', 'was', 'written']\n",
      "(3, 38)\n"
     ]
    }
   ],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "x=vectorizer.fit_transform(text_data)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>all</th>\n",
       "      <th>analytics</th>\n",
       "      <th>and</th>\n",
       "      <th>by</th>\n",
       "      <th>communication</th>\n",
       "      <th>context</th>\n",
       "      <th>customer</th>\n",
       "      <th>customers</th>\n",
       "      <th>data</th>\n",
       "      <th>drawing</th>\n",
       "      <th>examining</th>\n",
       "      <th>experience</th>\n",
       "      <th>for</th>\n",
       "      <th>in</th>\n",
       "      <th>include</th>\n",
       "      <th>is</th>\n",
       "      <th>meaning</th>\n",
       "      <th>means</th>\n",
       "      <th>methods</th>\n",
       "      <th>mining</th>\n",
       "      <th>of</th>\n",
       "      <th>or</th>\n",
       "      <th>out</th>\n",
       "      <th>prepare</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>process</th>\n",
       "      <th>processes</th>\n",
       "      <th>required</th>\n",
       "      <th>routines</th>\n",
       "      <th>tasks</th>\n",
       "      <th>text</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>those</th>\n",
       "      <th>to</th>\n",
       "      <th>was</th>\n",
       "      <th>written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.291726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282779</td>\n",
       "      <td>0.282779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.565558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309651</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.199366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      about       all  analytics       and        by  communication   context  \\\n",
       "0  0.000000  0.246968   0.000000  0.246968  0.000000       0.000000  0.000000   \n",
       "1  0.000000  0.000000   0.215061  0.000000  0.000000       0.282779  0.000000   \n",
       "2  0.262142  0.000000   0.199366  0.000000  0.262142       0.000000  0.262142   \n",
       "\n",
       "   customer  customers      data   drawing  examining  experience       for  \\\n",
       "0  0.000000   0.000000  0.246968  0.000000   0.000000    0.000000  0.246968   \n",
       "1  0.000000   0.000000  0.000000  0.282779   0.000000    0.000000  0.000000   \n",
       "2  0.262142   0.262142  0.000000  0.000000   0.262142    0.262142  0.000000   \n",
       "\n",
       "         in   include        is   meaning     means   methods    mining  \\\n",
       "0  0.000000  0.246968  0.000000  0.000000  0.000000  0.246968  0.246968   \n",
       "1  0.000000  0.000000  0.282779  0.282779  0.000000  0.000000  0.000000   \n",
       "2  0.262142  0.000000  0.000000  0.000000  0.262142  0.000000  0.000000   \n",
       "\n",
       "         of        or       out   prepare  preprocessing   process  processes  \\\n",
       "0  0.000000  0.000000  0.000000  0.246968       0.246968  0.000000   0.246968   \n",
       "1  0.565558  0.000000  0.282779  0.000000       0.000000  0.282779   0.000000   \n",
       "2  0.000000  0.262142  0.000000  0.000000       0.000000  0.000000   0.000000   \n",
       "\n",
       "   required  routines     tasks      text      that       the     those  \\\n",
       "0  0.246968  0.246968  0.246968  0.291726  0.000000  0.000000  0.246968   \n",
       "1  0.000000  0.000000  0.000000  0.167014  0.000000  0.282779  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.309651  0.262142  0.000000  0.000000   \n",
       "\n",
       "         to       was   written  \n",
       "0  0.246968  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.215061  \n",
       "2  0.000000  0.262142  0.199366  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "vector=x\n",
    "df1=pd.DataFrame(vector.toarray(),columns=vectorizer.get_feature_names())\n",
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
